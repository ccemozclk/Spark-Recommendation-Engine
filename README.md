# ðŸ›’ PySpark Retail Recommendation Engine & Customer Segmentation

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![PySpark](https://img.shields.io/badge/Apache%20Spark-PySpark-orange)
![Machine Learning](https://img.shields.io/badge/Machine%20Learning-ALS%20%26%20KMeans-green)

## ðŸ“– Project Overview

This project is an end-to-end data science implementation that leverages **Big Data** technologies to analyze large-scale retail datasets. The primary goal is to build a **Personalized Recommendation Engine** and perform **Customer Segmentation** to enhance marketing strategies and user experience.

Using the **PySpark** ecosystem, the project covers the entire pipeline: from data cleaning and feature engineering (RFM Analysis) to deploying machine learning models (K-Means Clustering & ALS Collaborative Filtering).

## ðŸŽ¯ Business Problem

In the competitive retail landscape, delivering the right product to the right customer at the right time is crucial for maximizing revenue (Cross-selling / Up-selling). This project addresses two key business challenges:
1.  **Customer Segmentation:** How can we group customers based on their purchasing behavior to identify VIPs, loyal customers, and those at risk of churn?
2.  **Personalization:** How can we recommend relevant products to users based on their implicit interactions, even if they haven't explicitly rated items?

## ðŸ› ï¸ Tech Stack & Methods

* **Platform:** Google Colab (Spark Environment)
* **Languages:** Python (PySpark, Pandas)
* **Libraries:** `pyspark.sql`, `pyspark.ml`
* **Algorithms:**
    * **K-Means Clustering:** For behavioral customer segmentation.
    * **ALS (Alternating Least Squares):** For collaborative filtering-based product recommendations.
* **Techniques:** RFM Analysis, StringIndexing, VectorAssembler, Pipeline Construction, Implicit Feedback Handling.

## ðŸ“Š Methodology (Step-by-Step)

### 1. Data Cleaning & Preprocessing
The Online Retail II dataset was processed to ensure high data quality:
* Resolved compatibility issues between Pandas `NaN` and Spark `Null` values.
* Filtered out cancelled transactions (Invoices starting with 'C') and outliers.
* Optimized data types (`String` -> `Timestamp`, `Integer`) for efficient processing.

### 2. Feature Engineering & RFM Analysis
Customer behavioral metrics were calculated to feed the segmentation model:
* **Recency:** Days since the last purchase.
* **Frequency:** Total number of transactions.
* **Monetary:** Total revenue generated by the customer.

### 3. Customer Segmentation (K-Means)
RFM metrics were standardized using `StandardScaler` to ensure equal weightage. The `K-Means` algorithm was applied to cluster customers into 5 distinct segments (e.g., Champions, At-Risk, New Customers).

### 4. Recommendation Engine (ALS)
* **Implicit Feedback:** Since retail data lacks explicit ratings (stars), purchase quantities were used as a proxy for user interest (`implicitPrefs=True`).
* **Model Training:** An Alternating Least Squares (ALS) model was trained on the User-Item interaction matrix.
* **Cold Start Strategy:** Applied a `drop` strategy to handle users present in the test set but missing from the training set.

## ðŸ“ˆ Results

The model successfully learned purchasing patterns and generated logical product recommendations.

**Sample Output:**
* **User 18286:** Identified as a buyer of patterned bags.
    * *Recommendations:* **"LUNCH BAG SPACEBOY DESIGN"**, **"LUNCH BAG BLACK SKULL"**.
* **User 18285:** Identified as a buyer of vintage/decorative items.
    * *Recommendations:* **"ANTIQUE SILVER TEA GLASS"**, **"VINTAGE CHRISTMAS KIT"**.

## ðŸŽ“ Certifications & Learning Path

This project serves as a practical application of the concepts mastered in the following **DataCamp** courses:
* ðŸ“œ Feature Engineering with PySpark
* ðŸ“œ Machine Learning with PySpark
* ðŸ“œ Building Recommendation Engines with PySpark

## ðŸš€ How to Run

1.  Clone this repository.
2.  Open the `.ipynb` file in Google Colab or a Databricks environment.
3.  Install PySpark:
    ```bash
    !pip install pyspark
    ```
4.  Run the cells sequentially (The dataset is automatically fetched from the UCI Repository).

---
*This project reflects my passion for Big Data technologies and solving real-world problems with Data Science.*
